{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "RNG = 2016\n",
    "\n",
    "# for language processing\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for classification\n",
    "from sklearn.metrics import (f1_score, log_loss, accuracy_score)\n",
    "from sklearn import (naive_bayes, ensemble, svm)\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from utils import evaluate_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series_summary</th>\n",
       "      <th>Series_title</th>\n",
       "      <th>label</th>\n",
       "      <th>label_code</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSE1001</th>\n",
       "      <td>Sprague-Dawley rat retina post-injury and cont...</td>\n",
       "      <td>retina injury timecourse</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10064</th>\n",
       "      <td>This study aims to determine if global gene ex...</td>\n",
       "      <td>Gene expression in immortalized B-lymphocytes ...</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10082</th>\n",
       "      <td>Conventional biochemical and molecular techniq...</td>\n",
       "      <td>Aryl Hydrocarbon Receptor Regulates Distinct D...</td>\n",
       "      <td>gene</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE1009</th>\n",
       "      <td>Gene expression profiling in glomeruli from hu...</td>\n",
       "      <td>Diabetic nephropathy</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE1010</th>\n",
       "      <td>RNA samples prepared from lymphoblastic cells ...</td>\n",
       "      <td>FCHL study</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Series_summary  \\\n",
       "id                                                            \n",
       "GSE1001   Sprague-Dawley rat retina post-injury and cont...   \n",
       "GSE10064  This study aims to determine if global gene ex...   \n",
       "GSE10082  Conventional biochemical and molecular techniq...   \n",
       "GSE1009   Gene expression profiling in glomeruli from hu...   \n",
       "GSE1010   RNA samples prepared from lymphoblastic cells ...   \n",
       "\n",
       "                                               Series_title label  label_code  \\\n",
       "id                                                                              \n",
       "GSE1001                            retina injury timecourse    dz           1   \n",
       "GSE10064  Gene expression in immortalized B-lymphocytes ...    dz           1   \n",
       "GSE10082  Aryl Hydrocarbon Receptor Regulates Distinct D...  gene           2   \n",
       "GSE1009                                Diabetic nephropathy    dz           1   \n",
       "GSE1010                                          FCHL study    dz           1   \n",
       "\n",
       "          split  \n",
       "id               \n",
       "GSE1001       0  \n",
       "GSE10064      0  \n",
       "GSE10082      0  \n",
       "GSE1009       0  \n",
       "GSE1010       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the labeled df\n",
    "df = pd.read_csv('data/Labeled_GSEs_texts_with_labels.csv').set_index('id')\n",
    "df = df.fillna('')\n",
    "print df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \"This study aims to determine if global gene expression and transcription factor networks in B-lympocytes of siblings with MS were different from healthy siblings. Keywords: Multiple sclerosis, sibling comparisons\"\n",
      "\n",
      "After tokenizing: ['This', 'study', 'aims', 'to', 'determine', 'if', 'global', 'gene', 'expression', 'and', 'transcription', 'factor', 'networks', 'in', 'lympocytes', 'of', 'siblings', 'with', 'MS', 'were', 'different', 'from', 'healthy', 'siblings', 'Keywords', 'Multiple', 'sclerosis', 'sibling', 'comparisons']\n",
      "\n",
      "After stemming: [u'Thi', u'studi', u'aim', u'to', u'determin', u'if', u'global', u'gene', u'express', u'and', u'transcript', u'factor', u'network', u'in', u'lympocyt', u'of', u'sibl', u'with', u'MS', u'were', u'differ', u'from', u'healthi', u'sibl', u'Keyword', u'Multipl', u'sclerosi', u'sibl', u'comparison']\n"
     ]
    }
   ],
   "source": [
    "# tokenize texts \n",
    "tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "doc = df.ix[1]['Series_summary']\n",
    "print 'Original document: \"%s\"' % doc\n",
    "\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "print '\\nAfter tokenizing:', tokens\n",
    "\n",
    "stems = [stemmer.stem(t) for t in tokens]\n",
    "print '\\nAfter stemming:',  stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_func = lambda x: ' '.join( tokenizer.tokenize(x) )\n",
    "df['Series_summary'] = df['Series_summary'].apply(preprocess_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "\n",
    "We use [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to count the words in each document and generate a sparse word count matrix with the shape = (n_documents, n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 115703)\n"
     ]
    }
   ],
   "source": [
    "# count tokens \n",
    "ctvec = CountVectorizer(min_df=1,\n",
    "                        max_df=0.8, # max document frequency, words with higher frequency than this will be ignored  \n",
    "                        max_features=None, \n",
    "                        strip_accents='unicode', \n",
    "                        decode_error='ignore',\n",
    "                        lowercase=True,\n",
    "                        tokenizer=None,\n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1, 2), # only keep uni-grams\n",
    "                        binary=True, # whether to return binary numbers or word counts\n",
    "                        stop_words='english')\n",
    "\n",
    "X = ctvec.fit_transform(df['Series_summary'])\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1           0.476865\n",
      "accuracy     0.612329\n",
      "logloss     13.248159\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = naive_bayes.BernoulliNB()\n",
    "y = df['label_code'].values\n",
    "\n",
    "scores = evaluate_clf(clf, X, y, df['split'])\n",
    "print scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 115701)\n"
     ]
    }
   ],
   "source": [
    "# count tokens \n",
    "ctvec = CountVectorizer(min_df=1,\n",
    "                        max_df=1000, # max document frequency, words with higher frequency than this will be ignored  \n",
    "                        max_features=None, \n",
    "                        strip_accents='unicode', \n",
    "                        decode_error='ignore',\n",
    "                        lowercase=True,\n",
    "                        tokenizer=None,\n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1, 2), # only keep uni-grams\n",
    "                        binary=False, # whether to return binary numbers or word counts\n",
    "                        stop_words='english')\n",
    "\n",
    "X = ctvec.fit_transform(df['Series_summary'])\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1          0.795405\n",
      "accuracy    0.807280\n",
      "logloss     3.095285\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clf = naive_bayes.MultinomialNB()\n",
    "\n",
    "scores = evaluate_clf(clf, X, y, df['split'])\n",
    "print scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent semantic analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 90)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=1, max_df=1000, \n",
    "                        max_features=None, strip_accents='unicode', \n",
    "                        decode_error='ignore',\n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1, 2), \n",
    "                        use_idf=True, smooth_idf=True, \n",
    "                        sublinear_tf=True, stop_words = 'english')\n",
    "svd = TruncatedSVD(n_components=90, algorithm='randomized', n_iter=5, random_state=RNG, tol=0.0)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('svd', svd)\n",
    "    ])\n",
    "\n",
    "X = pipeline.fit_transform(df['Series_summary'])\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1          0.809426\n",
      "accuracy    0.816796\n",
      "logloss     0.608025\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(n_estimators=1000, colsample_bytree=1, \n",
    "                         learning_rate=0.05, max_depth=8, subsample=0.9, \n",
    "                         min_child_weight=1, seed=RNG, nthread=4, silent=0)\n",
    "\n",
    "scores = evaluate_clf(clf, X, y, df['split'])\n",
    "print scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=4,\n",
      "            oob_score=False, random_state=2016, verbose=0,\n",
      "            warm_start=False)\n",
      "f1          0.751212\n",
      "accuracy    0.777590\n",
      "logloss     0.617417\n",
      "dtype: float64\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=1500, n_jobs=4,\n",
      "           oob_score=False, random_state=2016, verbose=0, warm_start=False)\n",
      "f1          0.751212\n",
      "accuracy    0.777590\n",
      "logloss     0.617417\n",
      "dtype: float64\n",
      "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=2016, shrinking=True,\n",
      "  tol=0.001, verbose=0)\n",
      "f1          0.751212\n",
      "accuracy    0.777590\n",
      "logloss     0.617417\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rf = ensemble.RandomForestClassifier(n_estimators=2000, criterion='entropy', \n",
    "                                     max_depth=None, random_state=RNG, n_jobs=4, verbose=0)\n",
    "et = ensemble.ExtraTreesClassifier(n_estimators=1500, criterion='entropy', max_depth=None, \n",
    "                                   random_state=RNG, n_jobs=4, verbose=0)\n",
    "svc = svm.SVC(C=100, kernel='rbf', probability=True, random_state=RNG, verbose=0)\n",
    "\n",
    "for clf in [rf, et, svc]:\n",
    "    scores = evaluate_clf(rf, X, y, df['split'])\n",
    "    print clf\n",
    "    print scores.mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
