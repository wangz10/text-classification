{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "RNG = 2016\n",
    "\n",
    "# for language processing\n",
    "from nltk.tokenize import (RegexpTokenizer, WordPunctTokenizer, TreebankWordTokenizer)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for classification\n",
    "from sklearn.metrics import (f1_score, log_loss, accuracy_score)\n",
    "from sklearn import (naive_bayes, ensemble, svm)\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from utils import evaluate_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series_summary</th>\n",
       "      <th>Series_title</th>\n",
       "      <th>label</th>\n",
       "      <th>label_code</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSE1001</th>\n",
       "      <td>Sprague-Dawley rat retina post-injury and cont...</td>\n",
       "      <td>retina injury timecourse</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10064</th>\n",
       "      <td>This study aims to determine if global gene ex...</td>\n",
       "      <td>Gene expression in immortalized B-lymphocytes ...</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10082</th>\n",
       "      <td>Conventional biochemical and molecular techniq...</td>\n",
       "      <td>Aryl Hydrocarbon Receptor Regulates Distinct D...</td>\n",
       "      <td>gene</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE1009</th>\n",
       "      <td>Gene expression profiling in glomeruli from hu...</td>\n",
       "      <td>Diabetic nephropathy</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE1010</th>\n",
       "      <td>RNA samples prepared from lymphoblastic cells ...</td>\n",
       "      <td>FCHL study</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Series_summary  \\\n",
       "id                                                            \n",
       "GSE1001   Sprague-Dawley rat retina post-injury and cont...   \n",
       "GSE10064  This study aims to determine if global gene ex...   \n",
       "GSE10082  Conventional biochemical and molecular techniq...   \n",
       "GSE1009   Gene expression profiling in glomeruli from hu...   \n",
       "GSE1010   RNA samples prepared from lymphoblastic cells ...   \n",
       "\n",
       "                                               Series_title label  label_code  \\\n",
       "id                                                                              \n",
       "GSE1001                            retina injury timecourse    dz           1   \n",
       "GSE10064  Gene expression in immortalized B-lymphocytes ...    dz           1   \n",
       "GSE10082  Aryl Hydrocarbon Receptor Regulates Distinct D...  gene           2   \n",
       "GSE1009                                Diabetic nephropathy    dz           1   \n",
       "GSE1010                                          FCHL study    dz           1   \n",
       "\n",
       "          split  \n",
       "id               \n",
       "GSE1001       0  \n",
       "GSE10064      0  \n",
       "GSE10082      0  \n",
       "GSE1009       0  \n",
       "GSE1010       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the labeled df\n",
    "df = pd.read_csv('data/Labeled_GSEs_texts_with_labels.csv').set_index('id')\n",
    "df = df.fillna('')\n",
    "print df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Basics\n",
    "\n",
    "## 1. Text normalizations\n",
    "\n",
    "###  1.1. Word Tokenization: segmenting words in running text\n",
    "+ **Token**: a word, phrase, symbol, or other meaningful elements in a running text.\n",
    "+ Tokenization is not a trival problem:\n",
    "    + Finland's capital\n",
    "    + What're, I'm, isn't\n",
    "    + state-of-the-art\n",
    "    + Lowercase\n",
    "    + New York\n",
    "    + Ph.D\n",
    "+ Other languages is even harder!\n",
    "    + **French**: l'ensemble -> un ensemble\n",
    "    + **German**: Rindfleischetikettierungsueberwachungsaufgabenuebertragungsgesetz, meaning \"law delegating beef label monitoring\", [the longest German word](http://www.bbc.com/news/world-europe-22762040).\n",
    "    + **Chinese**: 自然語言處理是人工智慧和語言學領域的分支學科   \n",
    "    `自然 語言 處理 是 人工智慧 和 語言學 領域 的 分支 學科`   \n",
    "    `Natural language processing is artificial intelligence and linguistics 's branch field`\n",
    "\n",
    "---\n",
    "\n",
    "Demonstration of different tokenization algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_text = \"\"\"\n",
    "Finland's capital, what're, I'm, state-of-the-art San Francisco Ph.D post-injury\n",
    "\"\"\"\n",
    "for tokenizer in [\n",
    "    RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\"), # white space tokenizer\n",
    "    WordPunctTokenizer(),\n",
    "    TreebankWordTokenizer()\n",
    "                 ]:\n",
    "    print '-' * 10\n",
    "    print tokenizer\n",
    "    print tokenizer.tokenize(example_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Word normalization\n",
    "+ **Lemma**: words with the same stem, e.g. `dog` and `dogs`.\n",
    "+ **Lemmatization**: reduce the variant forms to the base form\n",
    "    + dogs -> dog\n",
    "    + am, are, is -> be\n",
    "+ **Stem**: the core meaning-bearing unit of a word\n",
    "+ **Stemming**: reduce terms to their stems, a simpler form of Lemmatization.\n",
    "    + automates, automatic, automation -> automat\n",
    "\n",
    "+ Differences between **Lemmatization** and **Stemming** [(to read more)](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)   \n",
    "**Stemming** returns the stems of words.    \n",
    "**Lemmatization** returns the dictionary form of word.\n",
    "\n",
    "---\n",
    "\n",
    "Demonstration of stemming and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\") # white space tokenizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lmmr = WordNetLemmatizer()\n",
    "\n",
    "doc = df.ix[1]['Series_summary']\n",
    "print 'Original document: \\n\"%s\"' % doc\n",
    "\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "print '\\nAfter tokenizing:', tokens\n",
    "\n",
    "stems = [stemmer.stem(t) for t in tokens]\n",
    "print '\\nAfter stemming:',  stems\n",
    "\n",
    "lemmas = [lmmr.lemmatize(t) for t in tokens]\n",
    "print '\\nAfter lemantization:',  lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_func = lambda x: ' '.join( tokenizer.tokenize(x) )\n",
    "df['Series_summary'] = df['Series_summary'].apply(preprocess_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "\n",
    "We use [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to count the words in each document and generate a sparse word count matrix with the shape = (n_documents, n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count tokens \n",
    "ctvec = CountVectorizer(min_df=1,\n",
    "                        max_df=0.8, # max document frequency, words with higher frequency than this will be ignored  \n",
    "                        max_features=None, \n",
    "                        strip_accents='unicode', \n",
    "                        decode_error='ignore',\n",
    "                        lowercase=True,\n",
    "                        tokenizer=None,\n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1, 2), # only keep uni-grams\n",
    "                        binary=True, # whether to return binary numbers or word counts\n",
    "                        stop_words='english')\n",
    "\n",
    "X = ctvec.fit_transform(df['Series_summary'])\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = naive_bayes.BernoulliNB()\n",
    "y = df['label_code'].values\n",
    "\n",
    "scores = evaluate_clf(clf, X, y, df['split'])\n",
    "print scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count tokens \n",
    "ctvec = CountVectorizer(min_df=1,\n",
    "                        max_df=1000, # max document frequency, words with higher frequency than this will be ignored  \n",
    "                        max_features=None, \n",
    "                        strip_accents='unicode', \n",
    "                        decode_error='ignore',\n",
    "                        lowercase=True,\n",
    "                        tokenizer=None,\n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1, 2), # only keep uni-grams\n",
    "                        binary=False, # whether to return binary numbers or word counts\n",
    "                        stop_words='english')\n",
    "\n",
    "X = ctvec.fit_transform(df['Series_summary'])\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = naive_bayes.MultinomialNB()\n",
    "\n",
    "scores = evaluate_clf(clf, X, y, df['split'])\n",
    "print scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent semantic analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=1, max_df=1000, \n",
    "                        max_features=None, strip_accents='unicode', \n",
    "                        decode_error='ignore',\n",
    "                        analyzer='word', \n",
    "                        ngram_range=(1, 2), \n",
    "                        use_idf=True, smooth_idf=True, \n",
    "                        sublinear_tf=True, stop_words = 'english')\n",
    "svd = TruncatedSVD(n_components=90, algorithm='randomized', n_iter=5, random_state=RNG, tol=0.0)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('svd', svd)\n",
    "    ])\n",
    "\n",
    "X = pipeline.fit_transform(df['Series_summary'])\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(n_estimators=1000, colsample_bytree=1, \n",
    "                         learning_rate=0.05, max_depth=8, subsample=0.9, \n",
    "                         min_child_weight=1, seed=RNG, nthread=4, silent=0)\n",
    "\n",
    "scores = evaluate_clf(clf, X, y, df['split'])\n",
    "print scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier(n_estimators=2000, criterion='entropy', \n",
    "                                     max_depth=None, random_state=RNG, n_jobs=4, verbose=0)\n",
    "et = ensemble.ExtraTreesClassifier(n_estimators=1500, criterion='entropy', max_depth=None, \n",
    "                                   random_state=RNG, n_jobs=4, verbose=0)\n",
    "svc = svm.SVC(C=100, kernel='rbf', probability=True, random_state=RNG, verbose=0)\n",
    "\n",
    "for clf in [rf, et, svc]:\n",
    "    scores = evaluate_clf(rf, X, y, df['split'])\n",
    "    print clf\n",
    "    print scores.mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
