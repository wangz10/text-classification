{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph Vector for text classification\n",
    "---\n",
    "\n",
    "This notebook use the `gensim` implementation of Distributed Memory Model of Paragraph Vector (PV-DM). Some of the codes were taken from https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31905, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series_summary</th>\n",
       "      <th>Series_title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSE1</th>\n",
       "      <td>This series represents a group of cutaneous ma...</td>\n",
       "      <td>NHGRI_Melanoma_class</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE1000</th>\n",
       "      <td>Amino acid conjugated surfaces and controls at...</td>\n",
       "      <td>Osteosarcoma TE85 cell tissue culture study</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10000</th>\n",
       "      <td>We previously observed that formation of aorta...</td>\n",
       "      <td>Age-dependent aorta transcriptomes in wild-typ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10001</th>\n",
       "      <td>The thyroid hormone receptor (TR) has been pro...</td>\n",
       "      <td>Gene expression profiling in NCoR deficient mo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10002</th>\n",
       "      <td>Primitive erythropoiesis in the mouse yolk sac...</td>\n",
       "      <td>Identification of Erythroid-Enriched Gene Expr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Series_summary  \\\n",
       "id                                                            \n",
       "GSE1      This series represents a group of cutaneous ma...   \n",
       "GSE1000   Amino acid conjugated surfaces and controls at...   \n",
       "GSE10000  We previously observed that formation of aorta...   \n",
       "GSE10001  The thyroid hormone receptor (TR) has been pro...   \n",
       "GSE10002  Primitive erythropoiesis in the mouse yolk sac...   \n",
       "\n",
       "                                               Series_title label  \n",
       "id                                                                 \n",
       "GSE1                                   NHGRI_Melanoma_class   NaN  \n",
       "GSE1000         Osteosarcoma TE85 cell tissue culture study   NaN  \n",
       "GSE10000  Age-dependent aorta transcriptomes in wild-typ...   NaN  \n",
       "GSE10001  Gene expression profiling in NCoR deficient mo...   NaN  \n",
       "GSE10002  Identification of Erythroid-Enriched Gene Expr...   NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GSEs df\n",
    "df = pd.read_csv('data/GSEs_texts_with_labels.csv').set_index('id')\n",
    "df[['Series_summary', 'Series_title']] = df[['Series_summary', 'Series_title']].fillna('')\n",
    "print df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series_summary</th>\n",
       "      <th>Series_title</th>\n",
       "      <th>label</th>\n",
       "      <th>label_code</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GSE1001</th>\n",
       "      <td>Sprague-Dawley rat retina post-injury and cont...</td>\n",
       "      <td>retina injury timecourse</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10064</th>\n",
       "      <td>This study aims to determine if global gene ex...</td>\n",
       "      <td>Gene expression in immortalized B-lymphocytes ...</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE10082</th>\n",
       "      <td>Conventional biochemical and molecular techniq...</td>\n",
       "      <td>Aryl Hydrocarbon Receptor Regulates Distinct D...</td>\n",
       "      <td>gene</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE1009</th>\n",
       "      <td>Gene expression profiling in glomeruli from hu...</td>\n",
       "      <td>Diabetic nephropathy</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSE1010</th>\n",
       "      <td>RNA samples prepared from lymphoblastic cells ...</td>\n",
       "      <td>FCHL study</td>\n",
       "      <td>dz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Series_summary  \\\n",
       "id                                                            \n",
       "GSE1001   Sprague-Dawley rat retina post-injury and cont...   \n",
       "GSE10064  This study aims to determine if global gene ex...   \n",
       "GSE10082  Conventional biochemical and molecular techniq...   \n",
       "GSE1009   Gene expression profiling in glomeruli from hu...   \n",
       "GSE1010   RNA samples prepared from lymphoblastic cells ...   \n",
       "\n",
       "                                               Series_title label  label_code  \\\n",
       "id                                                                              \n",
       "GSE1001                            retina injury timecourse    dz           1   \n",
       "GSE10064  Gene expression in immortalized B-lymphocytes ...    dz           1   \n",
       "GSE10082  Aryl Hydrocarbon Receptor Regulates Distinct D...  gene           2   \n",
       "GSE1009                                Diabetic nephropathy    dz           1   \n",
       "GSE1010                                          FCHL study    dz           1   \n",
       "\n",
       "          split  \n",
       "id               \n",
       "GSE1001       0  \n",
       "GSE10064      0  \n",
       "GSE10082      0  \n",
       "GSE1009       0  \n",
       "GSE1010       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the labeled df\n",
    "df_labeled = pd.read_csv('data/Labeled_GSEs_texts_with_labels.csv').set_index('id')\n",
    "df_labeled = df_labeled.fillna('')\n",
    "print df_labeled.shape\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledDocument(tags=['GSE1'], words=['This', 'series', 'represents', 'group', 'of', 'cutaneous', 'malignant', 'melanomas', 'and', 'unrelated', 'controls', 'which', 'were', 'clustered', 'based', 'on', 'correlation', 'coefficients', 'calculated', 'through', 'comparison', 'of', 'gene', 'expression', 'profiles', 'Keywords', 'other'], label=nan)\n",
      "31905 documents collected.\n"
     ]
    }
   ],
   "source": [
    "# A short hand for defining a subclass for tuple, this is the required input format for gensim.doc2vec\n",
    "LabeledDocument = namedtuple('LabeledDocument', 'tags words label')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "for i, row in df.iterrows():\n",
    "# for i, row in df_labeled.iterrows():\n",
    "    tokens = tokenizer.tokenize(row['Series_summary'])\n",
    "    labeled_doc = LabeledDocument([i], tokens, row['label'])\n",
    "    if i == 'GSE1':\n",
    "        print labeled_doc\n",
    "    alldocs.append(labeled_doc)\n",
    "\n",
    "print '%d documents collected.' % len(alldocs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_models = []\n",
    "# PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "pvdm = Doc2Vec(dm=1, dm_concat=1, size=200, window=5, negative=5, hs=0, min_count=2, workers=8)\n",
    "# PV-DBOW \n",
    "# pvdbow = Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=8),\n",
    "# PV-DM w/average\n",
    "pvdm2 = Doc2Vec(dm=1, dm_mean=1, size=200, window=10, negative=5, hs=0, min_count=2, workers=8)\n",
    "\n",
    "# PV-DM/concat requires one special NULL word so it serves as template\n",
    "pvdm.build_vocab(alldocs)\n",
    "\n",
    "# sharing results of 1st model's vocabulary scan\n",
    "pvdm2.reset_from(pvdm)\n",
    "\n",
    "# from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "# concat_model = ConcatenatedDoc2Vec([pvdm, pvdm2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31905\n"
     ]
    }
   ],
   "source": [
    "doc_list = alldocs[:] # for reshuffling per pass\n",
    "print len(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Training\n",
    "\n",
    "Using explicit multiple-pass, alpha-reduction approach as sketched in gensim doc2vec blog post – with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on all documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the inferred results use newly-inferred TEST vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2016-05-24 16:45:13.895411\n",
      "completed pass 1 at alpha 0.025000\n",
      "completed pass 2 at alpha 0.023800\n",
      "completed pass 3 at alpha 0.022600\n",
      "completed pass 4 at alpha 0.021400\n",
      "completed pass 5 at alpha 0.020200\n",
      "completed pass 6 at alpha 0.019000\n",
      "completed pass 7 at alpha 0.017800\n",
      "completed pass 8 at alpha 0.016600\n",
      "completed pass 9 at alpha 0.015400\n",
      "completed pass 10 at alpha 0.014200\n",
      "completed pass 11 at alpha 0.013000\n",
      "completed pass 12 at alpha 0.011800\n",
      "completed pass 13 at alpha 0.010600\n",
      "completed pass 14 at alpha 0.009400\n",
      "completed pass 15 at alpha 0.008200\n",
      "completed pass 16 at alpha 0.007000\n",
      "completed pass 17 at alpha 0.005800\n",
      "completed pass 18 at alpha 0.004600\n",
      "completed pass 19 at alpha 0.003400\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2016-05-24 17:05:28.123754\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    # train\n",
    "    for model in [pvdm, pvdm2]:\n",
    "        model.alpha, concat_model.min_alpha = alpha, alpha\n",
    "        model.train(doc_list)\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3570, 100)\n"
     ]
    }
   ],
   "source": [
    "docvecs = concat_model.docvecs\n",
    "embedding_mat = docvecs[df_labeled.index] \n",
    "print embedding_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 400)\n"
     ]
    }
   ],
   "source": [
    "# Concat the paragraph embedding matrices from the two models\n",
    "embedding_mat = np.hstack((pvdm.docvecs[df_labeled.index], pvdm2.docvecs[df_labeled.index]))\n",
    "np.savetxt('models/pvdm_200_pvdm2_200_embedding.mat' ,embedding_mat)\n",
    "print embedding_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1785, 200)\n"
     ]
    }
   ],
   "source": [
    "# Load the concatenated embedding matrix\n",
    "embedding_mat = np.loadtxt('models/pvdm_100_pvdm2_100_embedding.mat')\n",
    "print embedding_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (f1_score, log_loss, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1          0.755071\n",
      "accuracy    0.761923\n",
      "logloss     0.658141\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l2')\n",
    "y = df_labeled['label_code'].values\n",
    "\n",
    "scores = evaluate_clf(clf, embedding_mat, y, df_labeled['split'])\n",
    "print scores.mean(axis=0) # accuracy  0.761923 (concat((pvdm_100, pvdm2_100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #10, avg. train loss: 0.95136\n",
      "Step #200, epoch #20, avg. train loss: 0.58923\n",
      "Step #300, epoch #30, avg. train loss: 0.46338\n",
      "Step #400, epoch #40, avg. train loss: 0.39758\n",
      "Step #500, epoch #50, avg. train loss: 0.35676\n",
      "Step #100, epoch #10, avg. train loss: 0.94522\n",
      "Step #200, epoch #20, avg. train loss: 0.57103\n",
      "Step #300, epoch #30, avg. train loss: 0.44425\n",
      "Step #400, epoch #40, avg. train loss: 0.37967\n",
      "Step #500, epoch #50, avg. train loss: 0.34232\n",
      "Step #100, epoch #10, avg. train loss: 0.96696\n",
      "Step #200, epoch #20, avg. train loss: 0.61894\n",
      "Step #300, epoch #30, avg. train loss: 0.50531\n",
      "Step #400, epoch #40, avg. train loss: 0.44441\n",
      "Step #500, epoch #50, avg. train loss: 0.39959\n",
      "f1          0.778588\n",
      "accuracy    0.784321\n",
      "logloss     0.545451\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "clf = learn.TensorFlowDNNClassifier(hidden_units=[100, 10], n_classes=3,\n",
    "    steps=500, learning_rate=0.01, batch_size=128)\n",
    "\n",
    "# 0.784892\n",
    "\n",
    "scores = evaluate_clf(clf, embedding_mat, y, df_labeled['split'])\n",
    "print scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1          0.706875\n",
      "accuracy    0.729408\n",
      "logloss     0.761081\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "RNG = 2016\n",
    "clf = xgb.XGBClassifier(n_estimators=1000, colsample_bytree=1, \n",
    "                         learning_rate=0.05, max_depth=8, subsample=0.9, \n",
    "                         min_child_weight=1, seed=RNG, nthread=4, silent=0)\n",
    "scores = evaluate_clf(clf, embedding_mat, y, df_labeled['split'])\n",
    "print scores.mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
